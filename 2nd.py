
"""

Automatically generated by Colab.

"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device='cuda' if torch.cuda.is_available() else'cpu'
device

model_name= 'gpt2-large'
tokenizer= AutoTokenizer.from_pretrained(model_name)

model= AutoModelForCausalLM.from_pretrained(model_name).to(device)

input_text='I am really impressed'
max_length =150
input_ids = tokenizer(input_text,return_tensors='pt')

input_ids

input_ids=input_ids['input_ids'].to(device)

output=model.generate(input_ids, max_length=max_length, do_sample=True, top_p=1)

output #encoded data

print(tokenizer.decode(output[0])) #decode data

